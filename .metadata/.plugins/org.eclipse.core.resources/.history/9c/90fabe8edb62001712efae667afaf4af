package samples
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka.KafkaUtils 
import org.apache.log4j.{Logger,Level}
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.streaming._
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.dstream.ForEachDStream
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import com.datastax.spark.connector._
import com.datastax.spark.connector.rdd._
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnector
import java.util.Date
import org.apache.spark.sql._
import org.apache.spark.sql.SQLContext
//import org.apache.spark.sql.SparkSession
object Test1 {
  def main(args : Array[String]) {
    
    Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    
    
    val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver") .set("spark.driver.allowMultipleContexts", "true").set("spark.cassandra.connection.host","localhost")        
val sc = new SparkContext(conf);
    val sqlContext = new SQLContext(sc);
val ssc = new StreamingContext(conf, Seconds(5))
    
 //CassandraConnector(sc).foreachRDD { session =>
   // session.execute(s"CREATE KEYSPACE IF NOT EXISTS streaming_test WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3 }")
    //session.execute(s"CREATE TABLE IF NOT EXISTS streaming_test.key_value (key VARCHAR PRIMARY KEY, value INT)")
    //session.execute(s"TRUNCATE streaming_test.key_value")
 // }

//val messages = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("nagesh" -> 1))

//val rdd1 = messages.map(_._2) // split the message into lines
      //.flatMap(_.split(",")) //split into words
      //.filter(w => (w.length() > 0))
      val rdd2 = rdd1.map(_.split(",")(6));//.map(z => (z(0), z(1), z(2), z(3), z(4), z(5), z(6).toInt))
      rdd2.count()
      
      val rdd3 = rdd1.map(_.split(",")(4));
     // val rdd4 = rdd3.join(rdd2);
      
  }
}