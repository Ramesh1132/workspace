package com.pgtest
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.sql.SQLContext
import com.datastax.spark.connector._
object ReadFromCassandra {
  
def main(args : Array[String]){
  val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver").set("spark.driver.allowMultipleContexts", "true").set("spark.cassandra.connection.host", "localhost")
    val sc = new SparkContext(conf);
    val sqlContext = new SQLContext(sc);
  val df = sqlContext.read.format("org.apache.spark.sql.cassandra") .options(Map( "table" -> "tot", "keyspace" -> "userdb" ))
  val a = df.load()
  val b = a.toDF()
  b.registerTempTable("temp")
 val df1 = sqlContext.read.format("org.apache.spark.sql.cassandra") .options(Map( "table" -> "tot1", "keyspace" -> "userdb" ))
 val c = df1.load()
 val d = c.toDF()
 d.registerTempTable("temp1")
//sqlContext.sql("SELECT * FROM temp t1 LEFT OUTER JOIN temp1 t2 ON t1.id = t2.id WHERE t2.id IS NULL ").show();
//sqlContext.sql("select * from (SELECT id,name,sal FROM temp minus SELECT id,name,sal FROM temp1").show()
//sqlContext.sql("SELECT id,name,sal from temp where (SELECT * FROM temp t1 LEFT OUTER JOIN temp1 t2 ON t1.id = t2.id WHERE t2.id IS NULL)").show();
  //val df3 = df.join(df1)
 sqlContext.sql("SELECT temp.* FROM temp LEFT JOIN temp1 ON (temp.id = temp1.id) WHERE temp1.id IS NULL").show();
  
 // df3.show()
  
}




}