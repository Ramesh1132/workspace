package com.pgtest

import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming._

import org.apache.spark.streaming.kafka.KafkaUtils 
import org.apache.log4j.{Logger,Level}
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.streaming._
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import com.datastax.spark.connector._
import com.datastax.spark.connector.rdd._
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnector
import java.util.Date
import org.apache.spark.sql._
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
//import kafka.serializer.StringDecoder
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.types.TimestampType
//import kafka.consumer.ConsumerConfig

import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.streaming.dstream.{ DStream, InputDStream }

import org.apache.spark.sql.types.DoubleType

import kafka.serializer.StringDecoder






object FinalCode {
  

  def main(args: Array[String]) {

Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    
    val schema = StructType(Array(
      StructField("cid", IntegerType, true),
      StructField("dt", TimestampType, true),
      StructField("lat", DoubleType, true),
      StructField("lon", DoubleType, true),
      StructField("base", StringType, true),
      StructField("clat", DoubleType, true),
      StructField("clon", DoubleType, true)
    ))

    val groupId = "testgroup"
    val offsetReset = "smallest"
    val pollTimeout = "5000"
    val Array(topicc) = args
    val brokers = "localhost:9092"


    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("UberStream2")
    .set("spark.streaming.kafka.maxRatePerPartition", "10")
    .set("spark.streaming.backpressure.enabled","true")
    .set("spark.sql.shuffle.partitions", "1")
    .set("spark.cassandra.connection.host", "localhost")
    .set("spark.cassandra.output.batch.size.bytes", "5120")
    .set("spark.cassandra.output.batch.size.rows", "1000")
    .set("spark.cassandra.output.concurrent.writes", "2")
    .set("spark.cassandra.output.consistency.level", "ANY")
   .set("spark.cassandra.output.batch.grouping.key", "none") ///replica_set/partition
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    val topicsSet = topicc.split(",").toSet

    val kafkaParams = Map[String, String](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokers,
      ConsumerConfig.GROUP_ID_CONFIG -> groupId,
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG ->
        "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG ->
        "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> offsetReset,
      ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -> "false",
      "spark.kafka.poll.time" -> pollTimeout
    )


    val messagesDStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
    val valuesDStream = messagesDStream.map(_._2)

    valuesDStream.foreachRDD(rdd => {
      val sqlContext = SQLContext.getOrCreate(ssc.sparkContext)
      import  sqlContext.implicits._

      //rdd.foreach(println)

      val df = sqlContext.read.schema(schema).json(rdd)

      //df.registerTempTable("ubermon")
      val df2 = df.withColumn("date", df("dt").cast(DateType))
      //val df2 = sqlContext.sql("SELECT cid, date(ubermon.dt) as date, dt, lat, lon, clat, clon, base from ubermon")
      //df2.registerTempTable("ubermon")
      println(df2.rdd.partitions.length)
      df2.write.mode(SaveMode.Append).format("org.apache.spark.sql.cassandra").options(Map( "table" -> "ubermonitor", "keyspace" -> "uber")).save()
    })

    ssc.start()
    ssc.awaitTermination()

    ssc.stop(stopSparkContext = true, stopGracefully = true)

  }
}
