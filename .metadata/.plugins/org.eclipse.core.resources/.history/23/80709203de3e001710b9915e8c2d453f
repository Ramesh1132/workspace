package com.pgtest.kafcal
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka.KafkaUtils 
import org.apache.log4j.{Logger,Level}
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.streaming._
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import com.datastax.spark.connector._
import com.datastax.spark.connector.rdd._
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnector
import java.util.Date
import org.apache.spark.sql._


case class stream3(provider_id :String, address: String, hospital_name : String)

object KafkaConsume {
  def main(args : Array[String]) {
    
    Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    
    
    val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver") .set("spark.driver.allowMultipleContexts", "true").set("spark.cassandra.connection.host","localhost")        

val ssc = new StreamingContext(conf, Seconds(5))
    
 

val kafkaStream = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("test" -> 1))



 // val wordCounts = 
    kafkaStream.map(_._2) // split the message into lines
      //.flatMap(_.split(",")) //split into words
      .filter(w => (w.length() > 0))
      .map(_.split(",")).map(z => (z(0), z(1), z(2)))
      .saveToCassandra("test", "stream3", SomeColumns("Provider ID","Address","Hospital Name"))// remove any empty words caused by double spaces
  ssc.start()
ssc.awaitTermination()







  }
}