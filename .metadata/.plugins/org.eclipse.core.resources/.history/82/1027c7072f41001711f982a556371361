package com.pgtest
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka.KafkaUtils 
import org.apache.log4j.{Logger,Level}
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.streaming._
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import com.datastax.spark.connector._
import com.datastax.spark.connector.rdd._
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnector
import java.util.Date
import org.apache.spark.sql._
import org.apache.spark.sql.SQLContext
object JsonData {
  Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    
    
    val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver") .set("spark.driver.allowMultipleContexts", "true").set("spark.cassandra.connection.host","localhost")        
val sc = new SparkContext(conf);
    val sqlContext = new SQLContext(sc);
val ssc = new StreamingContext(conf, Seconds(5))
    
 


    def parser(json: String): String = {
        ""
}
val messages = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("nagesh" -> 1))

val id = messages.map(_._2).map(parser)

       
val rdd1 = messages.map(_._2) // split the message into lines
.flatMap(_.split(",")) //split into words
.filter(w => (w.length() > 0))
.map(_.split(",")).map(z => (z(0), z(1), z(2), z(3), z(4), z(5), z(6)))

val rdd=  messages.foreachRDD { rdd =>
 val sqlContext = SQLContext.getOrCreate(rdd.sparkContext)
  import sqlContext.implicits._
// val js = sqlContext.read.json(rdd)
  val wordsDataFrame = rdd.toDF("id","f_name","l_name","email","gender","ip","salary")
wordsDataFrame.registerTempTable("words")
val wordCountsDataFrame = 
 sqlContext.sql("select * from words")
wordCountsDataFrame.show()
}


  ssc.start()
ssc.awaitTermination()

  }
